{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgIRruEYFPZb"
      },
      "source": [
        "# FuzConvTrue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3rC77OdFPZg",
        "outputId": "4d5fab3c-565b-4706-9ac0-f7bf273f2ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzup9mDsFPZi"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3aMaR_T-FPZj"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "from keras.layers import Activation\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras.layers import Lambda, Layer, ReLU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM, SpatialDropout2D, Concatenate\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, BatchNormalization\n",
        "from keras.layers.core import Reshape\n",
        "from keras import optimizers\n",
        "from tensorflow.keras import regularizers\n",
        "from keras import Input, Model\n",
        "from time import time\n",
        "import time as tm\n",
        "from keras.initializers import Constant, RandomNormal, glorot_normal\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVJpAPWFFPZk"
      },
      "source": [
        "## 30 SRM filters for preprocessing and the activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pnYLjyBaFPZk"
      },
      "outputs": [],
      "source": [
        "################################################## 30 SRM FILTERS\n",
        "srm_weights = np.load('drive/MyDrive/Numfiles/SRM.npy') \n",
        "biasSRM = np.ones(30)\n",
        "#print (srm_weights.shape)\n",
        "################################################## TANH ACTIVATION FUNCTION\n",
        "T3 = 3;\n",
        "def Tanh3(x):\n",
        "    tanh3 = K.tanh(x)*T3\n",
        "    return tanh3\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whMenLy_FPZl"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xhqyYSQ9FPZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98b842d-6933-4d1d-fb4f-7725582a60ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256)]        0         \n",
            "                                                                 \n",
            " tf.expand_dims_1 (TFOpLambd  (None, 256, 256, 1)      0         \n",
            " a)                                                              \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 62, 62, 96)        11712     \n",
            "                                                                 \n",
            " average_pooling2d_2 (Averag  (None, 30, 30, 96)       0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 30, 30, 256)       614656    \n",
            "                                                                 \n",
            " average_pooling2d_3 (Averag  (None, 14, 14, 256)      0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 14, 14, 384)       885120    \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 14, 14, 384)       1327488   \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 14, 14, 256)       884992    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4096)              37752832  \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 8194      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,484,994\n",
            "Trainable params: 41,484,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def FuzConvTrue(img_size=256):\n",
        "    inputs = Input(shape=(img_size, img_size))\n",
        "\n",
        "    Layers = tf.expand_dims(inputs, axis=-1)  #  channel dimension\n",
        "\n",
        "    Layers  = Conv2D(96, kernel_size=(11, 11), strides=(4, 4), padding='valid', activation='relu')(Layers )\n",
        "    Layers  = AveragePooling2D(pool_size=(3, 3), strides=(2, 2))(Layers )\n",
        "\n",
        "    Layers  = Conv2D(256, kernel_size=(5, 5), strides=(1, 1), padding='same', activation='relu')(Layers )\n",
        "    Layers  = AveragePooling2D(pool_size=(3, 3), strides=(2, 2))(Layers )\n",
        "\n",
        "    Layers  = Conv2D(384, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(Layers )\n",
        "\n",
        "    Layers  = Conv2D(384, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(Layers )\n",
        "\n",
        "    Layers  = Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(Layers )\n",
        "    Layers  = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(Layers )\n",
        "\n",
        "    Layers  = Flatten()(Layers )\n",
        "    Layers  = Dense(4096, activation='relu')(Layers )\n",
        "    Layers  = Dense(2, activation='softmax')(Layers )\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=Layers )\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = FuzConvTrue()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4shhW79VFPZo"
      },
      "source": [
        "## Defining different functions to work with the architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ncV9NgizFPZp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, initial_epoch=0, model_name=\"\"):\n",
        "    start_time = time()\n",
        "    log_dir = \"/content/drive/My Drive/FuzConvSteganalysis/trained_models/\" + model_name + \"_\" + \"{}\".format(time())\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir)\n",
        "    filepath = log_dir + \"/saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max')\n",
        "    model.reset_states()\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                        callbacks=[tensorboard,checkpoint], \n",
        "                        batch_size=batch_size, validation_data=(X_valid, y_valid), initial_epoch=initial_epoch)\n",
        "\n",
        "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
        "    results_dir = \"/content/drive/My Drive/FuzConvSteganalysis/Results/\" + model_name + \"/\"\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "      \n",
        "    with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        #plt.subplot(1,2,1)\n",
        "        #Plot training & validation accuracy values\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title('Accuracy Vs Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.grid('on')\n",
        "        plt.savefig(results_dir+'Accuracy_FuzConvTrue_'+model_name+'.eps', format='eps')\n",
        "        plt.savefig(results_dir+'Accuracy_FuzConvTrue_'+model_name+'.svg', format='svg')\n",
        "        plt.savefig(results_dir+'Accuracy_FuzConvTrue_'+model_name+'.pdf', format='pdf')\n",
        "        plt.show()\n",
        "        \n",
        "        plt.figure(figsize=(5, 5))\n",
        "        #plt.subplot(1,2,2)\n",
        "        #Plot training & validation loss values\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Loss Vs Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.grid('on')\n",
        "        plt.savefig(results_dir+'Loss_FuzConvTrue_'+model_name+'.eps', format='eps')\n",
        "        plt.savefig(results_dir+'Loss_FuzConvTrue_'+model_name+'.svg', format='svg')\n",
        "        plt.savefig(results_dir+'Loss_FuzConvTrue_'+model_name+'.pdf', format='pdf')\n",
        "        plt.show()\n",
        "\n",
        "        '''\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        #plt.subplot(1,2,2)\n",
        "        #Plot training & validation loss values\n",
        "        plt.plot(history.history['lr'])\n",
        "        plt.ylabel('Lr')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid('on')\n",
        "        plt.show()\n",
        "        '''\n",
        "    TIME = tm.time() - start_time\n",
        "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
        "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WvlHKMxhFPZq"
      },
      "outputs": [],
      "source": [
        "def Final_Results_Test(model,PATH_trained_models):\n",
        "    B_accuracy = 0 #B --> Best\n",
        "    for filename in os.listdir(PATH_trained_models):\n",
        "        if filename != ('train') and filename != ('validation'):\n",
        "            print(filename)\n",
        "            model.load_weights(PATH_trained_models+'/'+filename)\n",
        "            loss,accuracy = model.evaluate(X_test, y_test,verbose=0)\n",
        "            print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n') \n",
        "            if accuracy > B_accuracy:\n",
        "                B_accuracy = accuracy\n",
        "                B_loss = loss\n",
        "                B_name = filename\n",
        "    print(\"\\n\\nBest\")\n",
        "    print(B_name)\n",
        "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0g3tUZqQFPZr"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def plot_train_valid(model,PATH_trained_models,model_name):\n",
        "    acc_train=[]\n",
        "    acc_valid=[]\n",
        "    loss_train=[]\n",
        "    loss_valid=[]\n",
        "    for filename in tqdm(os.listdir(PATH_trained_models)):\n",
        "        if filename != ('train') and filename != ('validation'):\n",
        "            print(filename)\n",
        "            model.load_weights(PATH_trained_models+'/'+filename)\n",
        "            loss,accuracy = model.evaluate(X_train, y_train,verbose=0)\n",
        "            acc_train.append(accuracy)\n",
        "            loss_train.append(loss)\n",
        "            loss,accuracy = model.evaluate(X_valid, y_valid,verbose=0)\n",
        "            acc_valid.append(accuracy)\n",
        "            loss_valid.append(loss)\n",
        "\n",
        "    results_dir=\"/content/drive/My Drive/FuzConvSteganalysis/Results/\"+model_name+\"/\"\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "\n",
        "    with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        #plt.subplot(1,2,1)\n",
        "        #Plot training & validation accuracy values\n",
        "        plt.plot(acc_train)\n",
        "        plt.plot(acc_valid)\n",
        "        plt.title('Accuracy Vs Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.grid('on')\n",
        "        plt.savefig(results_dir+'FuzConvTrue_'+model_name+'.eps', format='eps')\n",
        "        plt.savefig(results_dir+'FuzConvTrue_'+model_name+'.svg', format='svg')\n",
        "        plt.savefig(results_dir+'FuzConvTrue_'+model_name+'.pdf', format='pdf')\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        #plt.subplot(1,2,2)\n",
        "        #Plot training & validation loss values\n",
        "        plt.plot(loss_train)\n",
        "        plt.plot(loss_valid)\n",
        "        plt.title('Loss Vs Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.grid('on')\n",
        "        plt.savefig(results_dir+'FuzConvTrue_'+model_name+'.eps', format='eps')\n",
        "        plt.savefig(results_dir+'FuzConvTrue_'+model_name+'.svg', format='svg')\n",
        "        plt.savefig(results_dir+'FuzConvTrue_'+model_name+'.pdf', format='pdf')\n",
        "        plt.show()\n",
        "\n",
        "        '''\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        #plt.subplot(1,2,2)\n",
        "        #Plot training & validation loss values\n",
        "        plt.plot(history.history['lr'])\n",
        "        plt.ylabel('Lr')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid('on')\n",
        "        plt.show()\n",
        "        '''\n",
        "    results={'acc_train':acc_train,'acc_valid':acc_valid,'loss_train':loss_train,'loss_valid':loss_valid}\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAFn8TQ-FPZs"
      },
      "source": [
        "## Plot ROC curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pURdd362FPZt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        ")\n",
        "\n",
        "def get_curve(gt, pred, target_names,model_name):\n",
        "    labels=[]\n",
        "    for i in range(len(target_names)):\n",
        "        \n",
        "        curve_function = roc_curve\n",
        "        auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n",
        "        label = model_name+target_names[i] + \" AUC: %.3f \" % auc_roc\n",
        "        labels.append(label)\n",
        "        xlabel = \"False positive rate\"\n",
        "        ylabel = \"True positive rate\"\n",
        "        a, b, _ = curve_function(gt[:, i], pred[:, i])\n",
        "        plt.figure(1, figsize=(7, 7))\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.plot(a, b, label=label)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "\n",
        "        plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
        "                  fancybox=True, ncol=1)\n",
        "      \n",
        "    return [a,b],labels\n",
        "labels = [\"innocent\",\"altered\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2siaub2SFPZt"
      },
      "source": [
        "## Working with BOSSbase 1.01 WOW PAYLOAD = 0.2bpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NPHVdwRFPZu",
        "outputId": "b1fd9d32-dbcd-4235-f5ea-98cbe9c69271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(46203, 256, 256)\n",
            "(46203, 2)\n",
            "(4297, 256, 256)\n",
            "(4297, 2)\n",
            "(5108, 256, 256)\n",
            "(5108, 2)\n"
          ]
        }
      ],
      "source": [
        "PATH04 = '/content/drive/My Drive'\n",
        "#Dataset\n",
        "EPOCHS=100\n",
        "PATH04_WOW1 = \"/Numfiles/\"\n",
        "\n",
        "#Train\n",
        "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
        "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
        "#Valid\n",
        "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
        "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
        "#Test\n",
        "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
        "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygxLPPfDFPZu"
      },
      "source": [
        "## CNN name and algorithm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l1r0i6BKFPZu"
      },
      "outputs": [],
      "source": [
        "base_name=\"FuzConvTrue\"\n",
        "m_name=\"FuzConvTrue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fiOlErOFPZv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= FuzConvTrue() \n",
        "name=\"Model_\"+m_name+\"_\"+base_name\n",
        "_, history  = train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=64, epochs=100, model_name=name)"
      ],
      "metadata": {
        "id": "U08Y0mJcepXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-joz4NOFPZv"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= FuzConvTrue() \n",
        "PATH_trained_models = 'drive/MyDrive/FuzConvSteganalysis/trained_models'\n",
        "Final_Results_Test(FuzConvTrue(),'drive/My Drive/FuzConvSteganalysis/trained_models/Model_FuzConvTrue_FuzConvTrue_1684741645.7035205')"
      ],
      "metadata": {
        "id": "eLl4CL3bevBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdXilRvlFPZw"
      },
      "source": [
        "## Training, validation and testing graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= FuzConvTrue()  \n",
        "PATH_trained_models = \"drive/My Drive/FuzConvSteganalysis/trained_models/Model_FuzConvTrue_FuzConvTrue_1684741645.7035205\"\n",
        "name=\"Model_\"+m_name+\"_\"+base_name\n",
        "res=plot_train_valid(model,PATH_trained_models,name)"
      ],
      "metadata": {
        "id": "NtOfz5R2f6bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSTNhxeAFPZx"
      },
      "source": [
        "## ROC curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= FuzConvTrue() \n",
        "model.load_weights(\"drive/My Drive/FuzConvSteganalysis/trained_models/Model_FuzConvTrue_FuzConvTrue_1684741645.7035205/saved-model-05-0.33.hdf5\") #path best model\n",
        "predictions= model.predict(X_test,verbose=0)\n",
        "labels = [\"innocent\",\"altered\"] \n",
        "model_name=\"FuzConvTrue \"\n",
        "curve1,labels1=get_curve(y_test, predictions, labels,model_name)"
      ],
      "metadata": {
        "id": "dWhEL6kif9Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-VrYW1bFPZx"
      },
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}